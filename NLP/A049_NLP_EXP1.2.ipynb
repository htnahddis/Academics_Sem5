{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7CzmR-FxwfGc",
        "outputId": "caac6d57-a7d1-4234-ac7b-408071aca2c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m80.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import re\n",
        "import string\n",
        "import spacy\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "!python -m spacy download en_core_web_sm\n",
        "sp=spacy.load('en_core_web_sm')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = 'i made a mistake'\n",
        "sentence.split()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JqRSLSHpYcM2",
        "outputId": "6fb8239b-b713-41f9-c9b2-60a801cd150f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i', 'made', 'a', 'mistake']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Sentence Tokenization using comma as a separator**"
      ],
      "metadata": {
        "id": "ERuRSdrBYtct"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = 'i made a mistake, but i realised it later'\n",
        "sentence.split(',')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7j3kcOWYupg",
        "outputId": "0fe3f738-3bf8-4793-d8d0-0a7049efc93c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i made a mistake', ' but i realised it later']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **NLTK Word Tokenize**"
      ],
      "metadata": {
        "id": "spfBC3ZIY0Od"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zhoKBW9cYs_C"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import(word_tokenize,sent_tokenize,TreebankWordTokenizer,PunktSentenceTokenizer,TweetTokenizer,MWETokenizer, wordpunct_tokenize)\n",
        "text = \"NLP combines computational linguistics‚Äîrule-based modeling of human language‚Äîwith statistical, machine learning, and deep learning models. Together, these technologies enable computers to process human language in the form of text or voice data and to ‚Äòunderstand‚Äô its full meaning, complete with the speaker or writer‚Äôs intent and sentiment\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R0pLNwFEYc0E",
        "outputId": "1ccf720a-b8ea-455b-a24d-7d8f2cb3ace5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(word_tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KInDGD2FY5XR",
        "outputId": "6eacea87-56b7-42af-e6f4-3ea4708ac863"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['NLP', 'combines', 'computational', 'linguistics‚Äîrule-based', 'modeling', 'of', 'human', 'language‚Äîwith', 'statistical', ',', 'machine', 'learning', ',', 'and', 'deep', 'learning', 'models', '.', 'Together', ',', 'these', 'technologies', 'enable', 'computers', 'to', 'process', 'human', 'language', 'in', 'the', 'form', 'of', 'text', 'or', 'voice', 'data', 'and', 'to', '‚Äò', 'understand', '‚Äô', 'its', 'full', 'meaning', ',', 'complete', 'with', 'the', 'speaker', 'or', 'writer', '‚Äô', 's', 'intent', 'and', 'sentiment']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Punkt Sentence Tokenizer**"
      ],
      "metadata": {
        "id": "CcR5rphuZS2m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(sent_tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzXd9be5Y7jy",
        "outputId": "98c6c1e5-f9c1-4cc0-96f1-647180ff98c1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['NLP combines computational linguistics‚Äîrule-based modeling of human language‚Äîwith statistical, machine learning, and deep learning models.', 'Together, these technologies enable computers to process human language in the form of text or voice data and to ‚Äòunderstand‚Äô its full meaning, complete with the speaker or writer‚Äôs intent and sentiment']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Punctuation Based Tokenizer**\n"
      ],
      "metadata": {
        "id": "19QYtS3NZa05"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(wordpunct_tokenize(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZ6g_DvIZWA_",
        "outputId": "c0aaa32b-28a7-40d1-ee68-e99e578de554"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['NLP', 'combines', 'computational', 'linguistics', '‚Äî', 'rule', '-', 'based', 'modeling', 'of', 'human', 'language', '‚Äî', 'with', 'statistical', ',', 'machine', 'learning', ',', 'and', 'deep', 'learning', 'models', '.', 'Together', ',', 'these', 'technologies', 'enable', 'computers', 'to', 'process', 'human', 'language', 'in', 'the', 'form', 'of', 'text', 'or', 'voice', 'data', 'and', 'to', '‚Äò', 'understand', '‚Äô', 'its', 'full', 'meaning', ',', 'complete', 'with', 'the', 'speaker', 'or', 'writer', '‚Äô', 's', 'intent', 'and', 'sentiment']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Treebank Word Tokenizer**\n"
      ],
      "metadata": {
        "id": "t-25xOAbZhJd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#tree bank\n",
        "text = \"why don't you come and ask again for it?\"\n",
        "t=TreebankWordTokenizer()\n",
        "print(t.tokenize(text))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0C2imAvZd2N",
        "outputId": "cf9bfb61-66a0-40bc-f8d4-b5d97809cc23"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['why', 'do', \"n't\", 'you', 'come', 'and', 'ask', 'again', 'for', 'it', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tweet Tokenizer**\n"
      ],
      "metadata": {
        "id": "qXrMqH_LZmbK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tweet=\"Don't take crypto advice from people on twitter ü§£ü§£\"\n",
        "tokenizer=TweetTokenizer()\n",
        "print(tokenizer.tokenize(tweet))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGp8mIutZjvY",
        "outputId": "2e9da4fe-ec77-4b14-ab12-da6f746fd70a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"Don't\", 'take', 'crypto', 'advice', 'from', 'people', 'on', 'twitter', 'ü§£', 'ü§£']\n"
          ]
        }
      ]
    }
  ]
}
{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: nltk in c:\\users\\siddhanth\\appdata\\roaming\\python\\python313\\site-packages (3.9.2)\n",
            "Requirement already satisfied: click in c:\\users\\siddhanth\\appdata\\roaming\\python\\python313\\site-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in c:\\users\\siddhanth\\appdata\\roaming\\python\\python313\\site-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\siddhanth\\appdata\\roaming\\python\\python313\\site-packages (from nltk) (2025.10.23)\n",
            "Requirement already satisfied: tqdm in c:\\users\\siddhanth\\appdata\\roaming\\python\\python313\\site-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: colorama in c:\\users\\siddhanth\\appdata\\roaming\\python\\python313\\site-packages (from click->nltk) (0.4.6)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.2 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting spacy\n",
            "  Downloading spacy-3.8.7-cp313-cp313-win_amd64.whl.metadata (28 kB)\n",
            "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
            "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
            "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
            "  Downloading murmurhash-1.0.13-cp313-cp313-win_amd64.whl.metadata (2.2 kB)\n",
            "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
            "  Downloading cymem-2.0.11-cp313-cp313-win_amd64.whl.metadata (8.8 kB)\n",
            "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
            "  Downloading preshed-3.0.10-cp313-cp313-win_amd64.whl.metadata (2.5 kB)\n",
            "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
            "  Downloading thinc-8.3.6-cp313-cp313-win_amd64.whl.metadata (15 kB)\n",
            "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
            "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
            "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
            "  Downloading srsly-2.5.1-cp313-cp313-win_amd64.whl.metadata (20 kB)\n",
            "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
            "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
            "  Downloading weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting typer<1.0.0,>=0.3.0 (from spacy)\n",
            "  Downloading typer-0.20.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\siddhanth\\appdata\\roaming\\python\\python313\\site-packages (from spacy) (4.67.1)\n",
            "Collecting numpy>=1.19.0 (from spacy)\n",
            "  Downloading numpy-2.3.4-cp313-cp313-win_amd64.whl.metadata (60 kB)\n",
            "Collecting requests<3.0.0,>=2.13.0 (from spacy)\n",
            "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spacy)\n",
            "  Downloading pydantic-2.12.3-py3-none-any.whl.metadata (87 kB)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\siddhanth\\appdata\\roaming\\python\\python313\\site-packages (from spacy) (3.1.5)\n",
            "Requirement already satisfied: setuptools in c:\\users\\siddhanth\\appdata\\roaming\\python\\python313\\site-packages (from spacy) (80.9.0)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\siddhanth\\appdata\\roaming\\python\\python313\\site-packages (from spacy) (25.0)\n",
            "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
            "  Downloading langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\n",
            "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
            "  Downloading language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting annotated-types>=0.6.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
            "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting pydantic-core==2.41.4 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
            "  Downloading pydantic_core-2.41.4-cp313-cp313-win_amd64.whl.metadata (7.4 kB)\n",
            "Collecting typing-extensions>=4.14.1 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
            "  Downloading typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting typing-inspection>=0.4.2 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
            "  Downloading typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting charset_normalizer<4,>=2 (from requests<3.0.0,>=2.13.0->spacy)\n",
            "  Downloading charset_normalizer-3.4.4-cp313-cp313-win_amd64.whl.metadata (38 kB)\n",
            "Collecting idna<4,>=2.5 (from requests<3.0.0,>=2.13.0->spacy)\n",
            "  Downloading idna-3.11-py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests<3.0.0,>=2.13.0->spacy)\n",
            "  Downloading urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting certifi>=2017.4.17 (from requests<3.0.0,>=2.13.0->spacy)\n",
            "  Downloading certifi-2025.10.5-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting blis<1.4.0,>=1.3.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
            "  Downloading blis-1.3.0-cp313-cp313-win_amd64.whl.metadata (7.6 kB)\n",
            "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy)\n",
            "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: colorama in c:\\users\\siddhanth\\appdata\\roaming\\python\\python313\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
            "Requirement already satisfied: click>=8.0.0 in c:\\users\\siddhanth\\appdata\\roaming\\python\\python313\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
            "Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
            "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting rich>=10.11.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
            "  Downloading rich-14.2.0-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
            "  Downloading cloudpathlib-0.23.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.1.0->spacy)\n",
            "  Downloading smart_open-7.4.1-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting wrapt (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy)\n",
            "  Downloading wrapt-2.0.0-cp313-cp313-win_amd64.whl.metadata (9.0 kB)\n",
            "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
            "  Downloading marisa_trie-1.3.1-cp313-cp313-win_amd64.whl.metadata (10 kB)\n",
            "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
            "  Downloading markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\siddhanth\\appdata\\roaming\\python\\python313\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
            "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
            "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\siddhanth\\appdata\\roaming\\python\\python313\\site-packages (from jinja2->spacy) (3.0.2)\n",
            "Downloading spacy-3.8.7-cp313-cp313-win_amd64.whl (13.9 MB)\n",
            "   ---------------------------------------- 0.0/13.9 MB ? eta -:--:--\n",
            "   -- ------------------------------------- 0.8/13.9 MB 12.8 MB/s eta 0:00:02\n",
            "   -- ------------------------------------- 0.8/13.9 MB 12.8 MB/s eta 0:00:02\n",
            "   -- ------------------------------------- 0.8/13.9 MB 12.8 MB/s eta 0:00:02\n",
            "   -- ------------------------------------- 0.8/13.9 MB 12.8 MB/s eta 0:00:02\n",
            "   ----- ---------------------------------- 1.8/13.9 MB 1.8 MB/s eta 0:00:07\n",
            "   ----- ---------------------------------- 1.8/13.9 MB 1.8 MB/s eta 0:00:07\n",
            "   ----- ---------------------------------- 1.8/13.9 MB 1.8 MB/s eta 0:00:07\n",
            "   ----- ---------------------------------- 1.8/13.9 MB 1.8 MB/s eta 0:00:07\n",
            "   --------- ------------------------------ 3.1/13.9 MB 1.6 MB/s eta 0:00:07\n",
            "   --------- ------------------------------ 3.1/13.9 MB 1.6 MB/s eta 0:00:07\n",
            "   --------- ------------------------------ 3.1/13.9 MB 1.6 MB/s eta 0:00:07\n",
            "   --------- ------------------------------ 3.1/13.9 MB 1.6 MB/s eta 0:00:07\n",
            "   --------- ------------------------------ 3.1/13.9 MB 1.6 MB/s eta 0:00:07\n",
            "   --------- ------------------------------ 3.1/13.9 MB 1.6 MB/s eta 0:00:07\n",
            "   --------- ------------------------------ 3.1/13.9 MB 1.6 MB/s eta 0:00:07\n",
            "   --------- ------------------------------ 3.1/13.9 MB 1.6 MB/s eta 0:00:07\n",
            "   --------- ------------------------------ 3.1/13.9 MB 1.6 MB/s eta 0:00:07\n",
            "   ------------ --------------------------- 4.5/13.9 MB 1.2 MB/s eta 0:00:09\n",
            "   -------------- ------------------------- 5.0/13.9 MB 1.3 MB/s eta 0:00:07\n",
            "   -------------- ------------------------- 5.0/13.9 MB 1.3 MB/s eta 0:00:07\n",
            "   --------------- ------------------------ 5.2/13.9 MB 1.2 MB/s eta 0:00:08\n",
            "   --------------- ------------------------ 5.2/13.9 MB 1.2 MB/s eta 0:00:08\n",
            "   --------------- ------------------------ 5.2/13.9 MB 1.2 MB/s eta 0:00:08\n",
            "   ---------------- ----------------------- 5.8/13.9 MB 1.1 MB/s eta 0:00:08\n",
            "   ------------------ --------------------- 6.6/13.9 MB 1.3 MB/s eta 0:00:06\n",
            "   ------------------ --------------------- 6.6/13.9 MB 1.3 MB/s eta 0:00:06\n",
            "   ------------------ --------------------- 6.6/13.9 MB 1.3 MB/s eta 0:00:06\n",
            "   ------------------ --------------------- 6.6/13.9 MB 1.3 MB/s eta 0:00:06\n",
            "   ------------------ --------------------- 6.6/13.9 MB 1.3 MB/s eta 0:00:06\n",
            "   ------------------ --------------------- 6.6/13.9 MB 1.3 MB/s eta 0:00:06\n",
            "   ------------------- -------------------- 6.8/13.9 MB 1.0 MB/s eta 0:00:07\n",
            "   --------------------- ------------------ 7.6/13.9 MB 1.1 MB/s eta 0:00:06\n",
            "   --------------------- ------------------ 7.6/13.9 MB 1.1 MB/s eta 0:00:06\n",
            "   ------------------------ --------------- 8.7/13.9 MB 1.2 MB/s eta 0:00:05\n",
            "   ------------------------- -------------- 8.9/13.9 MB 1.2 MB/s eta 0:00:05\n",
            "   ------------------------- -------------- 8.9/13.9 MB 1.2 MB/s eta 0:00:05\n",
            "   ------------------------- -------------- 8.9/13.9 MB 1.2 MB/s eta 0:00:05\n",
            "   ------------------------- -------------- 8.9/13.9 MB 1.2 MB/s eta 0:00:05\n",
            "   ------------------------- -------------- 8.9/13.9 MB 1.2 MB/s eta 0:00:05\n",
            "   ------------------------- -------------- 8.9/13.9 MB 1.2 MB/s eta 0:00:05\n",
            "   ------------------------- -------------- 8.9/13.9 MB 1.2 MB/s eta 0:00:05\n",
            "   ------------------------- -------------- 8.9/13.9 MB 1.2 MB/s eta 0:00:05\n",
            "   --------------------------- ------------ 9.7/13.9 MB 1.1 MB/s eta 0:00:04\n",
            "   ------------------------------ --------- 10.7/13.9 MB 1.2 MB/s eta 0:00:03\n",
            "   ------------------------------ --------- 10.7/13.9 MB 1.2 MB/s eta 0:00:03\n",
            "   -------------------------------- ------- 11.3/13.9 MB 1.2 MB/s eta 0:00:03\n",
            "   -------------------------------- ------- 11.3/13.9 MB 1.2 MB/s eta 0:00:03\n",
            "   -------------------------------- ------- 11.3/13.9 MB 1.2 MB/s eta 0:00:03\n",
            "   -------------------------------- ------- 11.3/13.9 MB 1.2 MB/s eta 0:00:03\n",
            "   ---------------------------------- ----- 12.1/13.9 MB 1.2 MB/s eta 0:00:02\n",
            "   ----------------------------------- ---- 12.3/13.9 MB 1.2 MB/s eta 0:00:02\n",
            "   ------------------------------------ --- 12.6/13.9 MB 1.2 MB/s eta 0:00:02\n",
            "   ------------------------------------ --- 12.6/13.9 MB 1.2 MB/s eta 0:00:02\n",
            "   ------------------------------------ --- 12.6/13.9 MB 1.2 MB/s eta 0:00:02\n",
            "   ---------------------------------------  13.6/13.9 MB 1.2 MB/s eta 0:00:01\n",
            "   ---------------------------------------  13.6/13.9 MB 1.2 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 13.9/13.9 MB 1.2 MB/s  0:00:11\n",
            "Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
            "Downloading cymem-2.0.11-cp313-cp313-win_amd64.whl (39 kB)\n",
            "Downloading langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
            "Downloading murmurhash-1.0.13-cp313-cp313-win_amd64.whl (24 kB)\n",
            "Downloading preshed-3.0.10-cp313-cp313-win_amd64.whl (115 kB)\n",
            "Downloading pydantic-2.12.3-py3-none-any.whl (462 kB)\n",
            "Downloading pydantic_core-2.41.4-cp313-cp313-win_amd64.whl (2.0 MB)\n",
            "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
            "   ----- ---------------------------------- 0.3/2.0 MB ? eta -:--:--\n",
            "   ----- ---------------------------------- 0.3/2.0 MB ? eta -:--:--\n",
            "   --------------- ------------------------ 0.8/2.0 MB 1.2 MB/s eta 0:00:02\n",
            "   -------------------- ------------------- 1.0/2.0 MB 1.4 MB/s eta 0:00:01\n",
            "   ------------------------- -------------- 1.3/2.0 MB 1.4 MB/s eta 0:00:01\n",
            "   ------------------------- -------------- 1.3/2.0 MB 1.4 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 1.8/2.0 MB 1.2 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 2.0/2.0 MB 1.2 MB/s  0:00:01\n",
            "Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "Downloading charset_normalizer-3.4.4-cp313-cp313-win_amd64.whl (107 kB)\n",
            "Downloading idna-3.11-py3-none-any.whl (71 kB)\n",
            "Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
            "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
            "Downloading srsly-2.5.1-cp313-cp313-win_amd64.whl (630 kB)\n",
            "   ---------------------------------------- 0.0/630.6 kB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/630.6 kB ? eta -:--:--\n",
            "   ---------------- ----------------------- 262.1/630.6 kB ? eta -:--:--\n",
            "   ---------------- ----------------------- 262.1/630.6 kB ? eta -:--:--\n",
            "   ---------------------------------------- 630.6/630.6 kB 743.0 kB/s  0:00:00\n",
            "Downloading thinc-8.3.6-cp313-cp313-win_amd64.whl (1.7 MB)\n",
            "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
            "   ------ --------------------------------- 0.3/1.7 MB ? eta -:--:--\n",
            "   ------------ --------------------------- 0.5/1.7 MB 1.4 MB/s eta 0:00:01\n",
            "   ------------------ --------------------- 0.8/1.7 MB 1.3 MB/s eta 0:00:01\n",
            "   ------------------ --------------------- 0.8/1.7 MB 1.3 MB/s eta 0:00:01\n",
            "   ------------------------ --------------- 1.0/1.7 MB 799.9 kB/s eta 0:00:01\n",
            "   ------------------------------------ --- 1.6/1.7 MB 1.2 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 1.7/1.7 MB 1.2 MB/s  0:00:01\n",
            "Downloading blis-1.3.0-cp313-cp313-win_amd64.whl (6.3 MB)\n",
            "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
            "   - -------------------------------------- 0.3/6.3 MB ? eta -:--:--\n",
            "   - -------------------------------------- 0.3/6.3 MB ? eta -:--:--\n",
            "   ------ --------------------------------- 1.0/6.3 MB 1.3 MB/s eta 0:00:05\n",
            "   -------- ------------------------------- 1.3/6.3 MB 1.3 MB/s eta 0:00:04\n",
            "   ---------- ----------------------------- 1.6/6.3 MB 1.3 MB/s eta 0:00:04\n",
            "   ---------- ----------------------------- 1.6/6.3 MB 1.3 MB/s eta 0:00:04\n",
            "   ---------- ----------------------------- 1.6/6.3 MB 1.3 MB/s eta 0:00:04\n",
            "   ---------- ----------------------------- 1.6/6.3 MB 1.3 MB/s eta 0:00:04\n",
            "   --------------- ------------------------ 2.4/6.3 MB 1.2 MB/s eta 0:00:04\n",
            "   ---------------- ----------------------- 2.6/6.3 MB 1.3 MB/s eta 0:00:03\n",
            "   ---------------- ----------------------- 2.6/6.3 MB 1.3 MB/s eta 0:00:03\n",
            "   ------------------ --------------------- 2.9/6.3 MB 1.1 MB/s eta 0:00:04\n",
            "   --------------------- ------------------ 3.4/6.3 MB 1.2 MB/s eta 0:00:03\n",
            "   ----------------------- ---------------- 3.7/6.3 MB 1.2 MB/s eta 0:00:03\n",
            "   ------------------------- -------------- 3.9/6.3 MB 1.2 MB/s eta 0:00:02\n",
            "   ------------------------- -------------- 3.9/6.3 MB 1.2 MB/s eta 0:00:02\n",
            "   ------------------------- -------------- 3.9/6.3 MB 1.2 MB/s eta 0:00:02\n",
            "   ------------------------- -------------- 3.9/6.3 MB 1.2 MB/s eta 0:00:02\n",
            "   ------------------------------- -------- 5.0/6.3 MB 1.2 MB/s eta 0:00:02\n",
            "   ------------------------------- -------- 5.0/6.3 MB 1.2 MB/s eta 0:00:02\n",
            "   ------------------------------- -------- 5.0/6.3 MB 1.2 MB/s eta 0:00:02\n",
            "   ------------------------------- -------- 5.0/6.3 MB 1.2 MB/s eta 0:00:02\n",
            "   --------------------------------- ------ 5.2/6.3 MB 1.1 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 6.0/6.3 MB 1.2 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 6.3/6.3 MB 1.2 MB/s  0:00:05\n",
            "Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
            "Downloading numpy-2.3.4-cp313-cp313-win_amd64.whl (12.8 MB)\n",
            "   ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
            "   -- ------------------------------------- 0.8/12.8 MB 2.7 MB/s eta 0:00:05\n",
            "   -- ------------------------------------- 0.8/12.8 MB 2.7 MB/s eta 0:00:05\n",
            "   --- ------------------------------------ 1.0/12.8 MB 1.9 MB/s eta 0:00:07\n",
            "   ---- ----------------------------------- 1.3/12.8 MB 1.7 MB/s eta 0:00:07\n",
            "   ---- ----------------------------------- 1.3/12.8 MB 1.7 MB/s eta 0:00:07\n",
            "   ---- ----------------------------------- 1.3/12.8 MB 1.7 MB/s eta 0:00:07\n",
            "   ------ --------------------------------- 2.1/12.8 MB 1.4 MB/s eta 0:00:08\n",
            "   ------- -------------------------------- 2.4/12.8 MB 1.5 MB/s eta 0:00:08\n",
            "   ------- -------------------------------- 2.4/12.8 MB 1.5 MB/s eta 0:00:08\n",
            "   ------- -------------------------------- 2.4/12.8 MB 1.5 MB/s eta 0:00:08\n",
            "   -------- ------------------------------- 2.6/12.8 MB 1.1 MB/s eta 0:00:10\n",
            "   ---------- ----------------------------- 3.4/12.8 MB 1.4 MB/s eta 0:00:07\n",
            "   ---------- ----------------------------- 3.4/12.8 MB 1.4 MB/s eta 0:00:07\n",
            "   ---------- ----------------------------- 3.4/12.8 MB 1.4 MB/s eta 0:00:07\n",
            "   ----------- ---------------------------- 3.7/12.8 MB 1.1 MB/s eta 0:00:09\n",
            "   ----------- ---------------------------- 3.7/12.8 MB 1.1 MB/s eta 0:00:09\n",
            "   ----------- ---------------------------- 3.7/12.8 MB 1.1 MB/s eta 0:00:09\n",
            "   ----------- ---------------------------- 3.7/12.8 MB 1.1 MB/s eta 0:00:09\n",
            "   ----------- ---------------------------- 3.7/12.8 MB 1.1 MB/s eta 0:00:09\n",
            "   ----------- ---------------------------- 3.7/12.8 MB 1.1 MB/s eta 0:00:09\n",
            "   ----------- ---------------------------- 3.7/12.8 MB 1.1 MB/s eta 0:00:09\n",
            "   ----------- ---------------------------- 3.7/12.8 MB 1.1 MB/s eta 0:00:09\n",
            "   ----------- ---------------------------- 3.7/12.8 MB 1.1 MB/s eta 0:00:09\n",
            "   ----------- ---------------------------- 3.7/12.8 MB 1.1 MB/s eta 0:00:09\n",
            "   ----------- ---------------------------- 3.7/12.8 MB 1.1 MB/s eta 0:00:09\n",
            "   ----------- ---------------------------- 3.7/12.8 MB 1.1 MB/s eta 0:00:09\n",
            "   ----------- ---------------------------- 3.7/12.8 MB 1.1 MB/s eta 0:00:09\n",
            "   ----------- ---------------------------- 3.7/12.8 MB 1.1 MB/s eta 0:00:09\n",
            "   ----------- ---------------------------- 3.7/12.8 MB 1.1 MB/s eta 0:00:09\n",
            "   ----------- ---------------------------- 3.7/12.8 MB 1.1 MB/s eta 0:00:09\n",
            "   ----------- ---------------------------- 3.7/12.8 MB 1.1 MB/s eta 0:00:09\n",
            "   ----------- ---------------------------- 3.7/12.8 MB 1.1 MB/s eta 0:00:09\n",
            "   ----------- ---------------------------- 3.7/12.8 MB 1.1 MB/s eta 0:00:09\n",
            "   ----------- ---------------------------- 3.7/12.8 MB 1.1 MB/s eta 0:00:09\n",
            "   ----------- ---------------------------- 3.7/12.8 MB 1.1 MB/s eta 0:00:09\n",
            "   ----------- ---------------------------- 3.7/12.8 MB 1.1 MB/s eta 0:00:09\n",
            "   ----------- ---------------------------- 3.7/12.8 MB 1.1 MB/s eta 0:00:09\n",
            "   ----------- ---------------------------- 3.7/12.8 MB 1.1 MB/s eta 0:00:09\n",
            "   ----------- ---------------------------- 3.7/12.8 MB 1.1 MB/s eta 0:00:09\n",
            "   ----------- ---------------------------- 3.7/12.8 MB 1.1 MB/s eta 0:00:09\n",
            "   ----------- ---------------------------- 3.7/12.8 MB 1.1 MB/s eta 0:00:09\n",
            "   ----------- ---------------------------- 3.7/12.8 MB 1.1 MB/s eta 0:00:09\n",
            "   ----------- ---------------------------- 3.7/12.8 MB 1.1 MB/s eta 0:00:09\n",
            "   ----------- ---------------------------- 3.7/12.8 MB 1.1 MB/s eta 0:00:09\n",
            "   ----------- ---------------------------- 3.7/12.8 MB 1.1 MB/s eta 0:00:09\n",
            "   ----------- ---------------------------- 3.7/12.8 MB 1.1 MB/s eta 0:00:09\n",
            "   ----------- ---------------------------- 3.7/12.8 MB 1.1 MB/s eta 0:00:09\n",
            "   ----------- ---------------------------- 3.7/12.8 MB 1.1 MB/s eta 0:00:09\n",
            "   ----------- ---------------------------- 3.7/12.8 MB 1.1 MB/s eta 0:00:09\n",
            "   ----------- ---------------------------- 3.7/12.8 MB 1.1 MB/s eta 0:00:09\n",
            "   ----------- ---------------------------- 3.7/12.8 MB 1.1 MB/s eta 0:00:09\n",
            "   ----------- ---------------------------- 3.7/12.8 MB 1.1 MB/s eta 0:00:09\n",
            "   ----------- ---------------------------- 3.7/12.8 MB 1.1 MB/s eta 0:00:09\n",
            "   ----------- ---------------------------- 3.7/12.8 MB 1.1 MB/s eta 0:00:09\n",
            "   ----------- ---------------------------- 3.7/12.8 MB 1.1 MB/s eta 0:00:09\n",
            "   ----------- ---------------------------- 3.7/12.8 MB 1.1 MB/s eta 0:00:09\n",
            "   ----------- ---------------------------- 3.7/12.8 MB 1.1 MB/s eta 0:00:09\n",
            "   ----------- ---------------------------- 3.7/12.8 MB 1.1 MB/s eta 0:00:09\n",
            "   ----------- ---------------------------- 3.7/12.8 MB 1.1 MB/s eta 0:00:09\n",
            "   ----------- ---------------------------- 3.7/12.8 MB 1.1 MB/s eta 0:00:09\n",
            "   ----------- ---------------------------- 3.7/12.8 MB 1.1 MB/s eta 0:00:09\n",
            "   ----------- ---------------------------- 3.7/12.8 MB 1.1 MB/s eta 0:00:09\n",
            "   ----------- ---------------------------- 3.7/12.8 MB 1.1 MB/s eta 0:00:09\n",
            "   ----------- ---------------------------- 3.7/12.8 MB 1.1 MB/s eta 0:00:09\n",
            "   ----------- ---------------------------- 3.7/12.8 MB 1.1 MB/s eta 0:00:09\n",
            "   ----------- ---------------------------- 3.7/12.8 MB 1.1 MB/s eta 0:00:09\n",
            "   ----------- ---------------------------- 3.7/12.8 MB 1.1 MB/s eta 0:00:09\n",
            "   ----------- ---------------------------- 3.7/12.8 MB 1.1 MB/s eta 0:00:09\n",
            "   ----------- ---------------------------- 3.7/12.8 MB 1.1 MB/s eta 0:00:09\n",
            "   ----------- ---------------------------- 3.7/12.8 MB 1.1 MB/s eta 0:00:09\n",
            "   ----------- ---------------------------- 3.7/12.8 MB 1.1 MB/s eta 0:00:09\n",
            "   ----------- ---------------------------- 3.7/12.8 MB 1.1 MB/s eta 0:00:09\n",
            "   ----------- ---------------------------- 3.7/12.8 MB 1.1 MB/s eta 0:00:09\n",
            "   ----------- ---------------------------- 3.7/12.8 MB 1.1 MB/s eta 0:00:09\n",
            "   ----------- ---------------------------- 3.7/12.8 MB 1.1 MB/s eta 0:00:09\n",
            "   ----------- ---------------------------- 3.7/12.8 MB 1.1 MB/s eta 0:00:09\n",
            "   ----------- ---------------------------- 3.7/12.8 MB 1.1 MB/s eta 0:00:09\n",
            "   ----------- ---------------------------- 3.7/12.8 MB 1.1 MB/s eta 0:00:09\n",
            "   ----------- ---------------------------- 3.7/12.8 MB 1.1 MB/s eta 0:00:09\n",
            "   ----------- ---------------------------- 3.7/12.8 MB 1.1 MB/s eta 0:00:09\n",
            "   ----------- ---------------------------- 3.7/12.8 MB 1.1 MB/s eta 0:00:09\n",
            "   ----------- ---------------------------- 3.7/12.8 MB 1.1 MB/s eta 0:00:09\n",
            "   ----------- ---------------------------- 3.7/12.8 MB 1.1 MB/s eta 0:00:09\n",
            "   ----------- ---------------------------- 3.7/12.8 MB 1.1 MB/s eta 0:00:09\n",
            "   ----------- ---------------------------- 3.7/12.8 MB 1.1 MB/s eta 0:00:09\n",
            "   ----------- ---------------------------- 3.7/12.8 MB 1.1 MB/s eta 0:00:09\n",
            "   ----------- ---------------------------- 3.7/12.8 MB 1.1 MB/s eta 0:00:09\n",
            "   ----------- ---------------------------- 3.7/12.8 MB 1.1 MB/s eta 0:00:09\n",
            "Resuming download numpy-2.3.4-cp313-cp313-win_amd64.whl (3.7 MB/12.8 MB)\n",
            "   ----------- ---------------------------- 3.7/12.8 MB ? eta -:--:--\n",
            "   ---------------- ----------------------- 5.2/12.8 MB 7.6 MB/s eta 0:00:01\n",
            "   ---------------- ----------------------- 5.2/12.8 MB 7.6 MB/s eta 0:00:01\n",
            "   ---------------- ----------------------- 5.2/12.8 MB 7.6 MB/s eta 0:00:01\n",
            "   ---------------- ----------------------- 5.2/12.8 MB 7.6 MB/s eta 0:00:01\n",
            "   ---------------- ----------------------- 5.2/12.8 MB 7.6 MB/s eta 0:00:01\n",
            "   ----------------- ---------------------- 5.5/12.8 MB 1.5 MB/s eta 0:00:05\n",
            "   ------------------ --------------------- 6.0/12.8 MB 1.7 MB/s eta 0:00:04\n",
            "   -------------------- ------------------- 6.6/12.8 MB 1.9 MB/s eta 0:00:04\n",
            "   -------------------- ------------------- 6.6/12.8 MB 1.9 MB/s eta 0:00:04\n",
            "   -------------------- ------------------- 6.6/12.8 MB 1.9 MB/s eta 0:00:04\n",
            "   ----------------------- ---------------- 7.6/12.8 MB 1.8 MB/s eta 0:00:03\n",
            "   ----------------------- ---------------- 7.6/12.8 MB 1.8 MB/s eta 0:00:03\n",
            "   ----------------------- ---------------- 7.6/12.8 MB 1.8 MB/s eta 0:00:03\n",
            "   ------------------------ --------------- 7.9/12.8 MB 1.5 MB/s eta 0:00:04\n",
            "   --------------------------- ------------ 8.7/12.8 MB 1.6 MB/s eta 0:00:03\n",
            "   --------------------------- ------------ 8.7/12.8 MB 1.6 MB/s eta 0:00:03\n",
            "   --------------------------- ------------ 8.7/12.8 MB 1.6 MB/s eta 0:00:03\n",
            "   --------------------------- ------------ 8.7/12.8 MB 1.6 MB/s eta 0:00:03\n",
            "   --------------------------- ------------ 8.7/12.8 MB 1.6 MB/s eta 0:00:03\n",
            "   --------------------------- ------------ 8.9/12.8 MB 1.3 MB/s eta 0:00:03\n",
            "   ------------------------------- -------- 10.0/12.8 MB 1.5 MB/s eta 0:00:02\n",
            "   ------------------------------- -------- 10.0/12.8 MB 1.5 MB/s eta 0:00:02\n",
            "   ------------------------------- -------- 10.0/12.8 MB 1.5 MB/s eta 0:00:02\n",
            "   ------------------------------- -------- 10.0/12.8 MB 1.5 MB/s eta 0:00:02\n",
            "   ------------------------------- -------- 10.0/12.8 MB 1.5 MB/s eta 0:00:02\n",
            "   ------------------------------- -------- 10.0/12.8 MB 1.5 MB/s eta 0:00:02\n",
            "   ---------------------------------- ----- 11.0/12.8 MB 1.3 MB/s eta 0:00:02\n",
            "   ---------------------------------- ----- 11.0/12.8 MB 1.3 MB/s eta 0:00:02\n",
            "   ---------------------------------- ----- 11.0/12.8 MB 1.3 MB/s eta 0:00:02\n",
            "   ------------------------------------ --- 11.8/12.8 MB 1.3 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 12.3/12.8 MB 1.4 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 12.3/12.8 MB 1.4 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 12.3/12.8 MB 1.4 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 12.3/12.8 MB 1.4 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 12.3/12.8 MB 1.4 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 12.8/12.8 MB 1.3 MB/s  0:00:07\n",
            "Downloading typer-0.20.0-py3-none-any.whl (47 kB)\n",
            "Downloading urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
            "Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
            "Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n",
            "Downloading cloudpathlib-0.23.0-py3-none-any.whl (62 kB)\n",
            "Downloading smart_open-7.4.1-py3-none-any.whl (63 kB)\n",
            "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
            "Downloading certifi-2025.10.5-py3-none-any.whl (163 kB)\n",
            "Downloading language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
            "   ---------------------------------------- 0.0/5.4 MB ? eta -:--:--\n",
            "   ------- -------------------------------- 1.0/5.4 MB 5.9 MB/s eta 0:00:01\n",
            "   --------- ------------------------------ 1.3/5.4 MB 4.2 MB/s eta 0:00:01\n",
            "   --------- ------------------------------ 1.3/5.4 MB 4.2 MB/s eta 0:00:01\n",
            "   --------- ------------------------------ 1.3/5.4 MB 4.2 MB/s eta 0:00:01\n",
            "   --------- ------------------------------ 1.3/5.4 MB 4.2 MB/s eta 0:00:01\n",
            "   ----------- ---------------------------- 1.6/5.4 MB 1.1 MB/s eta 0:00:04\n",
            "   ----------------- ---------------------- 2.4/5.4 MB 1.5 MB/s eta 0:00:02\n",
            "   ----------------- ---------------------- 2.4/5.4 MB 1.5 MB/s eta 0:00:02\n",
            "   ----------------- ---------------------- 2.4/5.4 MB 1.5 MB/s eta 0:00:02\n",
            "   ----------------- ---------------------- 2.4/5.4 MB 1.5 MB/s eta 0:00:02\n",
            "   ----------------- ---------------------- 2.4/5.4 MB 1.5 MB/s eta 0:00:02\n",
            "   ------------------- -------------------- 2.6/5.4 MB 988.2 kB/s eta 0:00:03\n",
            "   ------------------------- -------------- 3.4/5.4 MB 1.2 MB/s eta 0:00:02\n",
            "   --------------------------- ------------ 3.7/5.4 MB 1.3 MB/s eta 0:00:02\n",
            "   --------------------------- ------------ 3.7/5.4 MB 1.3 MB/s eta 0:00:02\n",
            "   ------------------------------- -------- 4.2/5.4 MB 1.2 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 5.0/5.4 MB 1.4 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 5.0/5.4 MB 1.4 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 5.0/5.4 MB 1.4 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 5.0/5.4 MB 1.4 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 5.0/5.4 MB 1.4 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 5.4/5.4 MB 1.2 MB/s  0:00:04\n",
            "Downloading marisa_trie-1.3.1-cp313-cp313-win_amd64.whl (139 kB)\n",
            "Downloading rich-14.2.0-py3-none-any.whl (243 kB)\n",
            "Downloading markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
            "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
            "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
            "Downloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
            "Downloading typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
            "Downloading wrapt-2.0.0-cp313-cp313-win_amd64.whl (60 kB)\n",
            "Installing collected packages: cymem, wrapt, wasabi, urllib3, typing-extensions, spacy-loggers, spacy-legacy, shellingham, numpy, murmurhash, mdurl, marisa-trie, idna, cloudpathlib, charset_normalizer, certifi, catalogue, annotated-types, typing-inspection, srsly, smart-open, requests, pydantic-core, preshed, markdown-it-py, language-data, blis, rich, pydantic, langcodes, typer, confection, weasel, thinc, spacy\n",
            "\n",
            "   --- ------------------------------------  3/35 [urllib3]\n",
            "   ---- -----------------------------------  4/35 [typing-extensions]\n",
            "   ------ ---------------------------------  6/35 [spacy-legacy]\n",
            "   --------- ------------------------------  8/35 [numpy]\n",
            "   --------- ------------------------------  8/35 [numpy]\n",
            "   --------- ------------------------------  8/35 [numpy]\n",
            "   --------- ------------------------------  8/35 [numpy]\n",
            "   --------- ------------------------------  8/35 [numpy]\n",
            "   --------- ------------------------------  8/35 [numpy]\n",
            "   --------- ------------------------------  8/35 [numpy]\n",
            "   --------- ------------------------------  8/35 [numpy]\n",
            "   --------- ------------------------------  8/35 [numpy]\n",
            "   --------- ------------------------------  8/35 [numpy]\n",
            "   --------- ------------------------------  8/35 [numpy]\n",
            "   --------- ------------------------------  8/35 [numpy]\n",
            "   --------- ------------------------------  8/35 [numpy]\n",
            "   --------- ------------------------------  8/35 [numpy]\n",
            "   --------- ------------------------------  8/35 [numpy]\n",
            "   --------- ------------------------------  8/35 [numpy]\n",
            "   --------- ------------------------------  8/35 [numpy]\n",
            "   --------- ------------------------------  8/35 [numpy]\n",
            "   --------- ------------------------------  8/35 [numpy]\n",
            "   --------- ------------------------------  8/35 [numpy]\n",
            "   --------- ------------------------------  8/35 [numpy]\n",
            "   ----------- ---------------------------- 10/35 [mdurl]\n",
            "   ------------- -------------------------- 12/35 [idna]\n",
            "   ---------------- ----------------------- 14/35 [charset_normalizer]\n",
            "   -------------------- ------------------- 18/35 [typing-inspection]\n",
            "   --------------------- ------------------ 19/35 [srsly]\n",
            "   --------------------- ------------------ 19/35 [srsly]\n",
            "   --------------------- ------------------ 19/35 [srsly]\n",
            "   ---------------------- ----------------- 20/35 [smart-open]\n",
            "   ------------------------- -------------- 22/35 [pydantic-core]\n",
            "   --------------------------- ------------ 24/35 [markdown-it-py]\n",
            "   --------------------------- ------------ 24/35 [markdown-it-py]\n",
            "   ---------------------------- ----------- 25/35 [language-data]\n",
            "   ---------------------------- ----------- 25/35 [language-data]\n",
            "   ---------------------------- ----------- 25/35 [language-data]\n",
            "   ---------------------------- ----------- 25/35 [language-data]\n",
            "   ---------------------------- ----------- 25/35 [language-data]\n",
            "   ---------------------------- ----------- 25/35 [language-data]\n",
            "   ---------------------------- ----------- 25/35 [language-data]\n",
            "   ----------------------------- ---------- 26/35 [blis]\n",
            "   ------------------------------ --------- 27/35 [rich]\n",
            "   ------------------------------ --------- 27/35 [rich]\n",
            "   -------------------------------- ------- 28/35 [pydantic]\n",
            "   -------------------------------- ------- 28/35 [pydantic]\n",
            "   -------------------------------- ------- 28/35 [pydantic]\n",
            "   -------------------------------- ------- 28/35 [pydantic]\n",
            "   --------------------------------- ------ 29/35 [langcodes]\n",
            "   ----------------------------------- ---- 31/35 [confection]\n",
            "   ------------------------------------ --- 32/35 [weasel]\n",
            "   ------------------------------------- -- 33/35 [thinc]\n",
            "   ------------------------------------- -- 33/35 [thinc]\n",
            "   ------------------------------------- -- 33/35 [thinc]\n",
            "   ------------------------------------- -- 33/35 [thinc]\n",
            "   -------------------------------------- - 34/35 [spacy]\n",
            "   -------------------------------------- - 34/35 [spacy]\n",
            "   -------------------------------------- - 34/35 [spacy]\n",
            "   -------------------------------------- - 34/35 [spacy]\n",
            "   -------------------------------------- - 34/35 [spacy]\n",
            "   -------------------------------------- - 34/35 [spacy]\n",
            "   -------------------------------------- - 34/35 [spacy]\n",
            "   -------------------------------------- - 34/35 [spacy]\n",
            "   -------------------------------------- - 34/35 [spacy]\n",
            "   -------------------------------------- - 34/35 [spacy]\n",
            "   -------------------------------------- - 34/35 [spacy]\n",
            "   -------------------------------------- - 34/35 [spacy]\n",
            "   -------------------------------------- - 34/35 [spacy]\n",
            "   -------------------------------------- - 34/35 [spacy]\n",
            "   -------------------------------------- - 34/35 [spacy]\n",
            "   -------------------------------------- - 34/35 [spacy]\n",
            "   -------------------------------------- - 34/35 [spacy]\n",
            "   -------------------------------------- - 34/35 [spacy]\n",
            "   -------------------------------------- - 34/35 [spacy]\n",
            "   -------------------------------------- - 34/35 [spacy]\n",
            "   -------------------------------------- - 34/35 [spacy]\n",
            "   ---------------------------------------- 35/35 [spacy]\n",
            "\n",
            "Successfully installed annotated-types-0.7.0 blis-1.3.0 catalogue-2.0.10 certifi-2025.10.5 charset_normalizer-3.4.4 cloudpathlib-0.23.0 confection-0.1.5 cymem-2.0.11 idna-3.11 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.3.1 markdown-it-py-4.0.0 mdurl-0.1.2 murmurhash-1.0.13 numpy-2.3.4 preshed-3.0.10 pydantic-2.12.3 pydantic-core-2.41.4 requests-2.32.5 rich-14.2.0 shellingham-1.5.4 smart-open-7.4.1 spacy-3.8.7 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.1 thinc-8.3.6 typer-0.20.0 typing-extensions-4.15.0 typing-inspection-0.4.2 urllib3-2.5.0 wasabi-1.1.3 weasel-0.4.1 wrapt-2.0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Connection timed out while downloading.\n",
            "WARNING: Attempting to resume incomplete download (3.7 MB/12.8 MB, attempt 1)\n",
            "\n",
            "[notice] A new release of pip is available: 25.2 -> 25.3\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk\n",
        "!pip install spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7CzmR-FxwfGc",
        "outputId": "4fa645fd-62b1-4e55-c3aa-058d77adbb65"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\Siddhanth\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\Siddhanth\\AppData\\Roaming\\nltk_data...\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import re\n",
        "import string\n",
        "import spacy\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "!python -m spacy download en_core_web_sm\n",
        "sp=spacy.load('en_core_web_sm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fj2T_RSAHzNy"
      },
      "outputs": [],
      "source": [
        "working_txt= \"Stemming is a method in text processing that       \\n eliminates prefixes and suffixes from words, transforming them into their fundamental or root form, The main objective of stemming is to streamline and standardize words, enhancing the effectiveness of the natural language processing tasks. The article explores more on the stemming technique and how to perform stemming in Python.What is Stemming in NLP?Simplifying words to their most basic form is called stemming, and it is made easier by stemmers or stemming algorithms. For example, “chocolates” becomes “chocolate” and “retrieval” becomes “retrieve.” This is crucial for pipelines for natural language processing, which use tokenized words that are acquired from the first stage of dissecting a document into its constituent words.Stemming in natural language processing reduces words to their base or root form, aiding in text normalization for easier processing. This technique is crucial in tasks like text classification, information retrieval, and text summarization. While beneficial, stemming has drawbacks, including potential impacts on text readability and occasional inaccuracies in determining the correct root form of a word.Why is Stemming important?It is important to note that stemming is different from Lemmatization. Lemmatization is the process of reducing a word to its base form, but unlike stemming, it takes into account the context of the word, and it produces a valid word, unlike stemming which may produce a non-word as the root form.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9KfCresH3mQ",
        "outputId": "a952c1cf-0d4c-489c-ccf9-54459aa23ac5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stemming is a method in text processing that eliminates prefixes and suffixes from words, transforming them into their fundamental or root form, The main objective of stemming is to streamline and standardize words, enhancing the effectiveness of the natural language processing tasks. The article explores more on the stemming technique and how to perform stemming in Python.What is Stemming in NLP?Simplifying words to their most basic form is called stemming, and it is made easier by stemmers or stemming algorithms. For example, “chocolates” becomes “chocolate” and “retrieval” becomes “retrieve.” This is crucial for pipelines for natural language processing, which use tokenized words that are acquired from the first stage of dissecting a document into its constituent words.Stemming in natural language processing reduces words to their base or root form, aiding in text normalization for easier processing. This technique is crucial in tasks like text classification, information retrieval, and text summarization. While beneficial, stemming has drawbacks, including potential impacts on text readability and occasional inaccuracies in determining the correct root form of a word.Why is Stemming important?It is important to note that stemming is different from Lemmatization. Lemmatization is the process of reducing a word to its base form, but unlike stemming, it takes into account the context of the word, and it produces a valid word, unlike stemming which may produce a non-word as the root form.\n"
          ]
        }
      ],
      "source": [
        "# clean text by removing successive whitespace and newlines then print regularized text\n",
        "clean_txt = re.sub(r\"\\n\", \" \", working_txt)\n",
        "clean_txt = re.sub(r\"\\s+\", \" \", clean_txt)\n",
        "clean_txt = clean_txt.strip()\n",
        "print(clean_txt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VENSto-8XHZa"
      },
      "source": [
        "Download tokenizer data package"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M56CgcngH79D",
        "outputId": "0708d81a-d455-4e59-8dba-b673ab5278ea"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VvZ7sHXXP9K"
      },
      "source": [
        "**Tokenization and Alphabetic Filtering**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "(.isalpha()), effectively removing punctuation like commas and periods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDSsE93xIA1f",
        "outputId": "0a872071-4696-482a-a8ba-cd2f7c6676e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Stemming', 'is', 'a', 'method', 'in', 'text', 'processing', 'that', 'eliminates', 'prefixes', 'and', 'suffixes', 'from', 'words', 'transforming', 'them', 'into', 'their', 'fundamental', 'or', 'root', 'form', 'The', 'main', 'objective', 'of', 'stemming', 'is', 'to', 'streamline', 'and', 'standardize', 'words', 'enhancing', 'the', 'effectiveness', 'of', 'the', 'natural', 'language', 'processing', 'tasks', 'The', 'article', 'explores', 'more', 'on', 'the', 'stemming', 'technique', 'and', 'how', 'to', 'perform', 'stemming', 'in', 'is', 'Stemming', 'in', 'NLP', 'Simplifying', 'words', 'to', 'their', 'most', 'basic', 'form', 'is', 'called', 'stemming', 'and', 'it', 'is', 'made', 'easier', 'by', 'stemmers', 'or', 'stemming', 'algorithms', 'For', 'example', 'chocolates', 'becomes', 'chocolate', 'and', 'retrieval', 'becomes', 'This', 'is', 'crucial', 'for', 'pipelines', 'for', 'natural', 'language', 'processing', 'which', 'use', 'tokenized', 'words', 'that', 'are', 'acquired', 'from', 'the', 'first', 'stage', 'of', 'dissecting', 'a', 'document', 'into', 'its', 'constituent', 'in', 'natural', 'language', 'processing', 'reduces', 'words', 'to', 'their', 'base', 'or', 'root', 'form', 'aiding', 'in', 'text', 'normalization', 'for', 'easier', 'processing', 'This', 'technique', 'is', 'crucial', 'in', 'tasks', 'like', 'text', 'classification', 'information', 'retrieval', 'and', 'text', 'summarization', 'While', 'beneficial', 'stemming', 'has', 'drawbacks', 'including', 'potential', 'impacts', 'on', 'text', 'readability', 'and', 'occasional', 'inaccuracies', 'in', 'determining', 'the', 'correct', 'root', 'form', 'of', 'a', 'is', 'Stemming', 'important', 'It', 'is', 'important', 'to', 'note', 'that', 'stemming', 'is', 'different', 'from', 'Lemmatization', 'Lemmatization', 'is', 'the', 'process', 'of', 'reducing', 'a', 'word', 'to', 'its', 'base', 'form', 'but', 'unlike', 'stemming', 'it', 'takes', 'into', 'account', 'the', 'context', 'of', 'the', 'word', 'and', 'it', 'produces', 'a', 'valid', 'word', 'unlike', 'stemming', 'which', 'may', 'produce', 'a', 'as', 'the', 'root', 'form']\n"
          ]
        }
      ],
      "source": [
        "# tokenize cleaned text\n",
        "tokens = word_tokenize(clean_txt)\n",
        "\n",
        "# remove non-alphabetic tokens and print\n",
        "filtered_tokens_alpha = [word for word in tokens if word.isalpha()]\n",
        "print(filtered_tokens_alpha)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_Wv9ZqIXpyX"
      },
      "source": [
        "**Stop Word Removal**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "This cell filters out common words that usually don't carry significant meaning, such as \"the,\" \"is,\" \"a,\" and \"in.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sYkdnRJEIDCV",
        "outputId": "7816f8e5-def6-413c-efc2-ebbb2b4677df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'theirs', 'has', 'below', 'its', 't', 'any', \"don't\", 's', 'shouldn', 'himself', \"they've\", \"she'll\", 'their', 'while', 'at', 'is', 'i', 'too', 'again', 'against', 'did', \"they're\", 'very', 'itself', 'if', 'down', \"weren't\", \"you'll\", 'themselves', 'd', 'here', 'over', 'under', 'hers', \"hadn't\", \"couldn't\", \"he's\", 'after', 'this', 'yourselves', \"haven't\", 'she', \"i'm\", 'mustn', 'he', 'doing', 'don', 'until', 'why', 'mightn', 'more', 're', 'some', \"that'll\", 'haven', 'it', \"shouldn't\", \"should've\", \"wouldn't\", 'wouldn', 'can', 'the', 'to', \"mightn't\", 'hadn', 'having', \"you'd\", 'ma', 'should', \"isn't\", \"mustn't\", 'weren', 'between', 'didn', 'o', 'ours', 'no', 'then', 'when', 'above', 'how', 'yourself', \"we're\", 'needn', 'wasn', 'be', 'myself', 'them', 'me', 'are', \"she'd\", 'her', 'only', \"she's\", 'was', \"i've\", 'own', 'most', \"aren't\", 've', 'am', 'aren', 'had', 'who', 'your', \"i'd\", 'were', 'we', 'doesn', 'those', 'with', 'off', 'my', 'where', 'than', \"won't\", \"we'll\", 'for', \"they'd\", \"you're\", 'hasn', \"they'll\", 'up', 'other', 'through', 'an', 'being', 'on', 'such', 'yours', 'as', 'isn', 'him', 'won', \"didn't\", 'but', 'couldn', 'ourselves', 'there', 'have', 'now', \"he'll\", 'by', 'before', 'so', 'that', \"shan't\", \"he'd\", 'what', \"it'll\", 'of', 'just', 'shan', 'these', 'll', 'once', 'ain', 'm', 'our', 'each', 'into', 'whom', 'all', \"doesn't\", 'few', 'do', 'not', \"hasn't\", 'from', 'his', 'or', 'herself', 'because', 'been', 'y', 'out', 'does', \"needn't\", 'during', \"we've\", 'you', \"you've\", \"it'd\", 'about', \"i'll\", 'same', 'will', 'a', 'in', 'which', \"it's\", 'they', \"we'd\", 'and', 'both', 'further', 'nor', \"wasn't\"}\n",
            "['Stemming', 'method', 'text', 'processing', 'eliminates', 'prefixes', 'suffixes', 'words', 'transforming', 'fundamental', 'root', 'form', 'The', 'main', 'objective', 'stemming', 'streamline', 'standardize', 'words', 'enhancing', 'effectiveness', 'natural', 'language', 'processing', 'tasks', 'The', 'article', 'explores', 'stemming', 'technique', 'perform', 'stemming', 'Stemming', 'NLP', 'Simplifying', 'words', 'basic', 'form', 'called', 'stemming', 'made', 'easier', 'stemmers', 'stemming', 'algorithms', 'For', 'example', 'chocolates', 'becomes', 'chocolate', 'retrieval', 'becomes', 'This', 'crucial', 'pipelines', 'natural', 'language', 'processing', 'use', 'tokenized', 'words', 'acquired', 'first', 'stage', 'dissecting', 'document', 'constituent', 'natural', 'language', 'processing', 'reduces', 'words', 'base', 'root', 'form', 'aiding', 'text', 'normalization', 'easier', 'processing', 'This', 'technique', 'crucial', 'tasks', 'like', 'text', 'classification', 'information', 'retrieval', 'text', 'summarization', 'While', 'beneficial', 'stemming', 'drawbacks', 'including', 'potential', 'impacts', 'text', 'readability', 'occasional', 'inaccuracies', 'determining', 'correct', 'root', 'form', 'Stemming', 'important', 'It', 'important', 'note', 'stemming', 'different', 'Lemmatization', 'Lemmatization', 'process', 'reducing', 'word', 'base', 'form', 'unlike', 'stemming', 'takes', 'account', 'context', 'word', 'produces', 'valid', 'word', 'unlike', 'stemming', 'may', 'produce', 'root', 'form']\n"
          ]
        }
      ],
      "source": [
        "# load stop list from NLTK\n",
        "stop_words = set(stopwords.words('english'))\n",
        "print(stop_words)\n",
        "# remove stop words from tokenized text and print\n",
        "filtered_tokens_final = [w for w in filtered_tokens_alpha if not w in stop_words]\n",
        "print(filtered_tokens_final)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94iekwsHX2hx"
      },
      "source": [
        "Stemming\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "This is the final processing step, where words are reduced to their root form, known as a \"stem.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGOB_Z39IFMA",
        "outputId": "44c8a6af-5d14-426d-8a2b-5ed8f45aa903"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['stem', 'method', 'text', 'process', 'elimin', 'prefix', 'suffix', 'word', 'transform', 'fundament', 'root', 'form', 'the', 'main', 'object', 'stem', 'streamlin', 'standard', 'word', 'enhanc', 'effect', 'natur', 'languag', 'process', 'task', 'the', 'articl', 'explor', 'stem', 'techniqu', 'perform', 'stem', 'stem', 'nlp', 'simplifi', 'word', 'basic', 'form', 'call', 'stem', 'made', 'easier', 'stemmer', 'stem', 'algorithm', 'for', 'exampl', 'chocol', 'becom', 'chocol', 'retriev', 'becom', 'thi', 'crucial', 'pipelin', 'natur', 'languag', 'process', 'use', 'token', 'word', 'acquir', 'first', 'stage', 'dissect', 'document', 'constitu', 'natur', 'languag', 'process', 'reduc', 'word', 'base', 'root', 'form', 'aid', 'text', 'normal', 'easier', 'process', 'thi', 'techniqu', 'crucial', 'task', 'like', 'text', 'classif', 'inform', 'retriev', 'text', 'summar', 'while', 'benefici', 'stem', 'drawback', 'includ', 'potenti', 'impact', 'text', 'readabl', 'occasion', 'inaccuraci', 'determin', 'correct', 'root', 'form', 'stem', 'import', 'it', 'import', 'note', 'stem', 'differ', 'lemmat', 'lemmat', 'process', 'reduc', 'word', 'base', 'form', 'unlik', 'stem', 'take', 'account', 'context', 'word', 'produc', 'valid', 'word', 'unlik', 'stem', 'may', 'produc', 'root', 'form']\n"
          ]
        }
      ],
      "source": [
        "# define Porter Stemmer from NLTK\n",
        "p_stemmer = PorterStemmer()\n",
        "\n",
        "# stem tokenized text and print first 500 tokens\n",
        "stemmed_tokens = [p_stemmer.stem(word) for word in filtered_tokens_final]\n",
        "print(stemmed_tokens[:500])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
